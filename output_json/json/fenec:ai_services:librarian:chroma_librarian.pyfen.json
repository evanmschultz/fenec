{
    "docstring": "",
    "header": [],
    "footer": [],
    "imports": [
        {
            "import_names": [
                {
                    "name": "logging",
                    "as_name": "",
                    "local_block_id": ""
                }
            ],
            "imported_from": "",
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": ""
        },
        {
            "import_names": [
                {
                    "name": "json",
                    "as_name": "",
                    "local_block_id": ""
                }
            ],
            "imported_from": "",
            "import_module_type": "STANDARD_LIBRARY",
            "local_module_id": ""
        },
        {
            "import_names": [
                {
                    "name": "OpenAI",
                    "as_name": "",
                    "local_block_id": ""
                }
            ],
            "imported_from": "openai",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": ""
        },
        {
            "import_names": [
                {
                    "name": "BaseModel",
                    "as_name": "",
                    "local_block_id": ""
                }
            ],
            "imported_from": "pydantic",
            "import_module_type": "THIRD_PARTY",
            "local_module_id": ""
        },
        {
            "import_names": [
                {
                    "name": "fenec.types.openai",
                    "as_name": "openai_types",
                    "local_block_id": ""
                }
            ],
            "imported_from": "",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:types:openai.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaCollectionManager",
                    "as_name": "",
                    "local_block_id": "fenec:databases:chroma:chromadb_collection_manager.py__*__MODULE__*__CLASS-ChromaCollectionManager"
                }
            ],
            "imported_from": "fenec.databases.chroma.chromadb_collection_manager",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:databases:chroma:chromadb_collection_manager.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "ChromaLibrarianPromptCreator",
                    "as_name": "",
                    "local_block_id": "fenec:ai_services:librarian:prompts:prompt_creator.py__*__MODULE__*__CLASS-ChromaLibrarianPromptCreator"
                }
            ],
            "imported_from": "fenec.ai_services.librarian.prompts.prompt_creator",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:librarian:prompts:prompt_creator.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_PROMPT",
                    "as_name": "",
                    "local_block_id": ""
                },
                {
                    "name": "DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT",
                    "as_name": "",
                    "local_block_id": ""
                }
            ],
            "imported_from": "fenec.ai_services.librarian.prompts.chroma_librarian_prompts",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:ai_services:librarian:prompts:chroma_librarian_prompts.py__*__MODULE"
        },
        {
            "import_names": [
                {
                    "name": "fenec.types.chroma",
                    "as_name": "chroma_types",
                    "local_block_id": ""
                }
            ],
            "imported_from": "",
            "import_module_type": "LOCAL",
            "local_module_id": "fenec:types:chroma.py__*__MODULE"
        }
    ],
    "id": "fenec:ai_services:librarian:chroma_librarian.py__*__MODULE",
    "file_path": "fenec/ai_services/librarian/chroma_librarian.py",
    "parent_id": "fenec:ai_services:librarian__*__DIRECTORY",
    "block_type": "MODULE",
    "start_line_num": 1,
    "end_line_num": 195,
    "code_content": "import logging\nimport json\n\nfrom openai import OpenAI\nfrom pydantic import BaseModel\nimport fenec.types.openai as openai_types\nfrom fenec.databases.chroma.chromadb_collection_manager import (\n    ChromaCollectionManager,\n)\nfrom fenec.ai_services.librarian.prompts.prompt_creator import (\n    ChromaLibrarianPromptCreator,\n)\nfrom fenec.ai_services.librarian.prompts.chroma_librarian_prompts import (\n    DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n    DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n)\nimport fenec.types.chroma as chroma_types\n\n# TOOLS: list[dict[str, Any]] = [\n#     {\n#         \"type\": \"function\",\n#         \"function\": {\n#             \"name\": \"query_chroma\",\n#             \"description\": \"Get the results from the chromadb vector database using a list of queries.\",\n#             \"parameters\": {\n#                 \"type\": \"object\",\n#                 \"properties\": {\n#                     \"queries\": {\n#                         \"type\": \"list[str]\",\n#                         \"description\": \"List of queries to use to get the results from the chromadb vector database.\",\n#                     },\n#                     \"n_results\": {\n#                         \"type\": \"int\",\n#                         \"description\": \"Number of results to return, default is 10.\",\n#                     },\n#                 },\n#                 \"required\": [\"queries\"],\n#             },\n#         },\n#     }\n# ]\n\n\nclass OpenAIResponseContent(BaseModel):\n    \"\"\"\n    Pydantic model representing the content structure of an OpenAI response.\n\n    OpenAI is set to respond with a JSON object, so this model is used to parse the response.\n\n    Attributes:\n        - query_list (list[str]): List of queries in the OpenAI response.\n    \"\"\"\n\n    query_list: list[str]\n\n\nclass ChromaLibrarian:\n    def __init__(\n        self,\n        collection_manager: ChromaCollectionManager,\n        model: str = \"gpt-3.5-turbo-1106\",\n    ) -> None:\n        \"\"\"\n        Represents a librarian for interacting with the Chroma database using OpenAI.\n\n        Args:\n            - collection_manager (ChromaCollectionManager): The manager for Chroma collections.\n            - model (str, optional): The OpenAI model to use. Defaults to \"gpt-3.5-turbo-1106\".\n\n        Methods:\n            - query_chroma(user_question):\n                Queries the Chroma database using the provided user question.\n\n            - _query_collection(queries, n_results=3):\n                Queries the Chroma collection manager with a list of queries.\n\n            - _get_chroma_queries(user_question, queries_count=3, retries=3):\n                Generates Chroma queries based on the user question.\n\n        Attributes:\n            - collection_manager (ChromaCollectionManager): The Chroma collection manager.\n            - model (str): The OpenAI model being used.\n            - client: The OpenAI API client.\n\n        Examples:\n            ```python\n            chroma_librarian = ChromaLibrarian(chroma_collection_manager)\n            chroma_librarian.query_chroma(\"Which models are inherited by others?\")\n            ```\n        \"\"\"\n\n        self.collection_manager: ChromaCollectionManager = collection_manager\n        self.model: str = model\n        self.client = OpenAI()\n\n    def query_chroma(self, user_question: str) -> chroma_types.QueryResult | None:\n        \"\"\"\n        Queries the Chroma database using the provided user question.\n\n        Args:\n            - user_question (str): The user's question.\n\n        Returns:\n            - chroma_types.QueryResult | None: The result of the Chroma query, or None if unsuccessful.\n        \"\"\"\n\n        queries: list[str] | None = self._get_chroma_queries(user_question)\n        if not queries:\n            return None\n\n        print(queries)\n\n        return self._query_collection(queries)\n\n    def _query_collection(\n        self,\n        queries: list[str],\n        n_results: int = 3,\n    ) -> chroma_types.QueryResult | None:\n        \"\"\"\n        Queries the Chroma collection manager with a list of queries.\n\n        Args:\n            - queries (list[str]): List of queries to use in the Chroma collection manager.\n            - n_results (int, optional): Number of results to return. Defaults to 3.\n\n        Returns:\n            - chroma_types.QueryResult | None: The result of the Chroma query, or None if unsuccessful.\n        \"\"\"\n\n        return self.collection_manager.query_collection(\n            queries,\n            n_results=n_results,\n            include_in_result=[\"metadatas\", \"documents\"],\n        )\n\n    def _get_chroma_queries(\n        self, user_question: str, queries_count: int = 3, retries: int = 3\n    ) -> list[str] | None:\n        \"\"\"\n        Generates Chroma queries based on the user question.\n\n        Args:\n            - user_question (str): The user's question.\n            - queries_count (int, optional): Number of queries to generate. Defaults to 3.\n            - retries (int, optional): Number of retries in case of failure. Defaults to 3.\n\n        Returns:\n            - list[str] | None: The generated list of Chroma queries, or None if unsuccessful.\n        \"\"\"\n\n        while retries > 0:\n            retries -= 1\n\n            prompt: str = ChromaLibrarianPromptCreator.create_prompt(\n                user_question,\n                prompt_template=DEFAULT_CHROMA_LIBRARIAN_PROMPT,\n                queries_count=queries_count,\n            )\n\n            try:\n                completion: openai_types.ChatCompletion = (\n                    self.client.chat.completions.create(\n                        model=self.model,\n                        response_format={\"type\": \"json_object\"},\n                        messages=[\n                            {\n                                \"role\": \"system\",\n                                \"content\": DEFAULT_CHROMA_LIBRARIAN_SYSTEM_PROMPT,\n                            },\n                            {\"role\": \"user\", \"content\": prompt},\n                        ],\n                    )\n                )\n                content: str | None = completion.choices[0].message.content\n                if not content:\n                    continue\n\n                content_json = json.loads(content)\n                content_model = OpenAIResponseContent(\n                    query_list=content_json[\"query_list\"]\n                )\n                content_model.query_list.append(user_question)\n                queries_count += 1\n\n                if content:\n                    queries: list[str] = content_model.query_list\n                    if queries and len(queries) == queries_count:\n                        return queries\n\n            except Exception as e:\n                logging.error(f\"An error occurred: {e}\")\n\n        return None\n",
    "important_comments": [],
    "dependencies": [],
    "summary": "This code is designed to enhance the querying capabilities of a Chroma vector database by integrating OpenAI's language model to transform user input into structured database queries. Its primary purpose is to facilitate intelligent and contextually relevant data retrieval from ChromaDB, thereby improving user interaction with the database. The main components include the `OpenAIResponseContent` class, a Pydantic model that ensures the JSON response from OpenAI is correctly parsed into a list of queries; and the `ChromaLibrarian` class, which orchestrates the entire process of query generation and execution. The `ChromaLibrarian` class features methods such as `query_chroma` for processing user questions into database queries, `_query_collection` for executing these queries against the Chroma collection manager, and `_get_chroma_queries` for generating queries using OpenAI's API with a retry mechanism to handle potential failures.\n\nThe implementation follows a structured approach where user questions are first converted into prompts using the `ChromaLibrarianPromptCreator`. These prompts are then sent to OpenAI's API to generate potential database queries. The system employs a retry loop within `_get_chroma_queries` to ensure robustness, attempting multiple times if initial attempts fail. The generated queries are executed against the Chroma database using the `ChromaCollectionManager`, which manages the database collections and retrieves results, including metadata and documents, based on the queries. This process is facilitated by the facade design pattern, which simplifies interactions with the complex subsystems of the OpenAI API and Chroma database.\n\nThe technical stack includes the OpenAI API, which provides the language model capabilities for generating and refining queries; Pydantic, used for data validation and parsing of OpenAI's JSON responses; and custom modules from the Fenec library, such as `ChromaCollectionManager` for database management and `ChromaLibrarianPromptCreator` for creating prompts. JSON is used for parsing API responses, and the OpenAI API client is instantiated within the class to facilitate communication with the language model.\n\nIn the context of a larger system, this code acts as an intermediary layer that translates natural language questions into actionable database queries, integrating with other components like the ChromaCollectionManager and prompt creation utilities. It is part of a broader architecture that aims to provide a seamless and intelligent querying experience, potentially interfacing with user-facing applications or services that require efficient data retrieval from vector databases. This integration allows for more intuitive data access and manipulation, leveraging AI to improve the accuracy and relevance of query results, and is likely part of a larger application involving AI-driven data analysis or retrieval.",
    "children_ids": [
        "fenec:ai_services:librarian:chroma_librarian.py__*__MODULE__*__CLASS-OpenAIResponseContent",
        "fenec:ai_services:librarian:chroma_librarian.py__*__MODULE__*__CLASS-ChromaLibrarian"
    ]
}